= Release Notes Generation Tool (RLGEN)
:author: Pantelis Tzamalis
:email: tzamalis@ceid.upatras.gr

== Issues




== Release Note: httpd 2.4.37 GA

* Total Jiras: 31

==== JBCS-787

* Summary: Rebase curl to 7.64.1

* Description:

============================

libcurl should be rebased to 7.64.1 from 7.57 that currently is

============================

==== JBCS-776

* Summary: Make sure the httpd.conf files location of 2.4.37 are similar to the 2.4.29

* Description:

============================

Some new modules have been added:
mod_md
mod_lua

============================

==== JBCS-775

* Summary: using deprecated CRYPTO_set_id_callback for OpenSSL

* Description:

============================

After applying 2.4.29 SP2, the following notice is being generated upon startup to httpd.log:
{code}
[ssl:notice] [pid 6625] AH10028: using deprecated CRYPTO_set_id_callback for OpenSSL
{code}

============================

==== JBCS-763

* Summary: Upgrade mod_cluster (native) to 1.3.11.Final-redhat

* Description:

============================

None

============================

==== JBCS-748

* Summary: JWS3.0-Optional rpms in the EWS installation guide for httpd zip

* Description:

============================

Customer notice that the links in the "lib" directory of EWS 3.0 httpd zip installation, points to missing files. For example:

{noformat}
lrwxrwxrwx. 1 root root  40 Aug 22 13:25 apr_dbd_pgsql-1.so -> /usr/lib64/apr-util-1/apr_dbd_pgsql-1.so
lrwxrwxrwx. 1 root root  38 Aug 22 13:25 apr_dbd_pgsql.so -> /usr/lib64/apr-util-1/apr_dbd_pgsql.so
{noformat}

"apr-util-pgsql" package is not required for httpd service to run, however it does contain the binary to satisfy the link.   

Customer suggest it should be mention  JBoss Web Server 3.0 installation guide. I think possible a "optional rpms" section,  thats mention "apr-devel , apr-util-ldap , apr-util-pgsql , apr-util-odbc ,  apr-util-mysql , apr-util-openssl and  apr-util-nss " package to satisfy the link



============================

==== JBCS-740

* Summary: JON Apache plugin fails to discover JBCS Apache HTTP with SP1 applied

* Description:

============================

Discovery of Red Hat JBoss Core Services Apache HTTP Server 2.4.29 fails after the Patch 01 is applied to the server.

Prior to Patch 1, the {{httpd -V}} of JBCS Apache is {{Server version: Apache/2.4.29 (Red Hat)}} but after applying SP1 for jbcs 2.4.29 the {{Server version: JBCS httpd/2.4.29-SP1-35 (Red Hat)}}.

So discovery fails with:

{noformat}
[ResourceDiscoveryComponent.invoker.daemon-2] (org.rhq.plugins.apache.ApacheServerDiscoveryComponent)- Discovery of Apache process [process: pid=[23893], name=[/NotBackedUp/jbcs-httpd24-2.4/httpd/sbin/httpd], ppid=[1]] failed: Apache null is not supported.
{noformat}

============================

==== JBCS-739

* Summary: JBCS SP1 changed the server header

* Description:

============================

After applied SP1 patch to JBCS 2.4.29 changed the server header from Apache to JBCS:

~~~
Server version: JBCS httpd/2.4.29-SP1-35 (Red Hat)
Server built:   Nov 23 2018 16:02:23
~~~

With ServerTokens Prod, the server header remains as:

~~~
Server: JBCS httpd
~~~

The header is disclosing vendor information, should be good if revert to "Server: Apache".


============================

==== JBCS-735

* Summary: Errata for httpd 2.4.37 GA RHEL 7

* Description:

============================

None

============================

==== JBCS-729

* Summary: mod_cluster routing mix up after upgrade to 2.4.29

* Description:

============================

modcluster seems to routing the request to a different node, the log is showing request goes to one node but the response is from another node result in 404.

============================

==== JBCS-718

* Summary: mod_proxy_hcheck Doesn't perform checks

* Description:

============================

Added the following config and restart the server:

{code}
<VirtualHost _default_:80>
ServerName localhost
    ProxyPass /test balancer://testAP/ stickysession=JSESSIONID
    ProxyPassReverse /test balancer://testAP/

    <Proxy balancer://testAP>
        BalancerMember http://www.redhat.com:80 loadfactor=10 hcmethod=TCP hcinterval=10 hcpasses=2 hcfails=3
        BalancerMember http://www.jboss.org:80 loadfactor=10 hcmethod=TCP hcinterval=10 hcpasses=2 hcfails=3
    </Proxy>

    LogLevel proxy_hcheck:TRACE8
    LogLevel proxy:TRACE8
    LogLevel watchdog:TRACE8
</VirtualHost>
{code}

But health check does not work

There is the following similar bug filed at :
bz: https://bz.apache.org/bugzilla/show_bug.cgi?id=60757
svn: https://svn.apache.org/viewvc?view=revision&revision=1792905

But, it does not resolves the issue. Looks addtional fix.

============================

==== JBCS-717

* Summary: Impossible to disable insertion of header=expect=100-Continue in proxied requests

* Description:

============================

* httpd/mod_proxy inserts a header expect=100-Continue in the proxied requests to EAP. This apparently happens consistently for all http POST requests (i.e. not GET).
* This header insertion did not occur in httpd 2.2; it is new in 2.4 (mentioned in mod_proxy documentation for proxying over HTTP as the "ping" feature). 
* It seems impossible to prevent the header insertion, neither through httpd config (e.g. via the ProxyPass ping=0 option), nor via the EAP mod_cluster subsystem config (ping option there as well).
* EAP responds almost immediately with an intermediary status HTTP 100 (proven through httpd debug logs which show the reception of this HTTP 100).
* When EAP does not answer "on time" to the actual request (body) e.g. with a HTTP 200 or other, httpd reports:
** [proxy:error] AH00898: Timeout on 100-Continue returned by (the URL)
** and then [:error] proxy: CLUSTER: (balancer://clstcmbalancer). All workers are in error state
** thereafter rejecting all requests from all users until the worker is made back available after some time.

============================

==== JBCS-712

* Summary: Missing apr dependencies when install jbcs-httpd24-httpd package 

* Description:

============================

Installation of the standalone  jbcs-httpd24-httpd-2.4.29-17.jbcs.el7.x86_64 package fails to install the jbcs-httpd24-apr and the  jbcs-httpd24-apr-util package as dependencies which causes Apache service fail during startup until the dependencies are resolved. 

============================

==== JBCS-710

* Summary: Failover scenario is not performed with httpd balancer - balancer fails to respond

* Description:

============================

 When second request in failover scenario is made to check whether the failover was done, server fails to respond. With Undertow as a balancer, this behavior cannot be reproduced. When I replace the jvmroute part by worker which is still alive, I got expected response.

{noformat}# curl -v --cookie "JSESSIONID=54yxdncGr5im0fBLqIIUMon0klbS66X16aYC_cVW.jboss-eap-7.2-3" http://172.17.0.2:2080/clusterbench/jvmroute
* About to connect() to 172.17.0.2 port 2080 (#0)
*   Trying 172.17.0.2...
* Connected to 172.17.0.2 (172.17.0.2) port 2080 (#0)
> GET /clusterbench/jvmroute HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 172.17.0.2:2080
> Accept: */*
> Cookie: JSESSIONID=54yxdncGr5im0fBLqIIUMon0klbS66X16aYC_cVW.jboss-eap-7.2-3
>
* Empty reply from server
* Connection #0 to host 172.17.0.2 left intact
curl: (52) Empty reply from server
{noformat}

============================

==== JBCS-695

* Summary: Apache httpd with worker/event mpm segfaults after multiple successive graceful reloads triggered by logrotate

* Description:

============================

Apache httpd in RHEL7 configured with worker/event mpm  with multiple logrotate segfaults after receiving multiple successive graceful reloads (SIGHUP).


============================

==== JBCS-634

* Summary: High CPU in mod_cluster with high httpd VirtualHost counts when restarting JBoss instances

* Description:

============================

If a configuration contains many VirtualHosts (100+), there is notable CPU spikes seen in httpd processes when JBoss instances stop/start and cause balancer changes.  It looks like there is O(n^2) complexity here where n is the VirtualHost counts.  

add_balancers_workers loops for each VirtualHost and calls create_worker each time. Note that with CreateBalancers 2, the other vhosts always have a balancer once it's populatedi n the main_server, so this check never skips any work for the vhosts:
{code}
        if (!balancer && (creat_bal == CREAT_NONE ||
            (creat_bal == CREAT_ROOT && s!=main_server))) {
            s = s->next;
            continue;
        }
{code}

So add_balancers_workers is O( n ).  proxy_cluster_watchdog_func and proxy_cluster_child_init call update_workers_node/add_balancers_workers for each vhost as well for n^2 complexity.

============================

==== JBCS-630

* Summary: Missing dependencies on jbcs-httpd24-httpd-2.4.29-17.jbcs.el7.x86_64

* Description:

============================

After installing jbcs-httpd24-httpd-2.4.29-17.jbcs.el7.x86_64 package using yum, the jbcs service fails with message "/opt/rh/jbcs-httpd24/root/usr/sbin/httpd: error while loading shared libraries: libaprutil-1.so.0: cannot open shared object file: No such file or directory"


============================

==== JBCS-586

* Summary: PCRE was built without utf-32 support

* Description:

============================

calling pcretest -32
(using support for utf-32 is required for testing linked JIRAs)

prints 
{noformat}
** ** This version of PCRE was built without 32-bit support
{noformat}

============================

==== JBCS-539

* Summary: SSLOCSPEnable setting is not inherited from server config into vhost config

* Description:

============================

Hello, test for:
{noformat}
JBCS-241 SSLOCSPEnable setting is not inherited from server config into vhost config
{noformat}
is failing. If I add 'SSLOCSPEnable On' directly to VirtualHost section it passes again.

============================

==== JBCS-535

* Summary: CheckCaseOnly On does not stop Multiple Choices based on common basename

* Description:

============================

Tests for https://issues.jboss.org/browse/JBCS-57 are failing again...did we forget to backport the patch?

============================

==== JBCS-507

* Summary: [GSS][2.4.23.x] Apache CacheMaxExpire directive ignored

* Description:

============================

    Content is cached, but doesn't expire in the time specified by Location +CacheMaxExpire+, but rather as specified in +CacheDefaultExpire+.
    If CacheMaxExpire (in Location directive) is replaced by CacheDefaultExpire, cache expires as expected.
CacheMaxExpire doesn't have effect when configured with the Location directive?
Old bugzillas 
* [https://bz.apache.org/bugzilla/show_bug.cgi?id=21260|https://bz.apache.org/bugzilla/show_bug.cgi?id=21260]
* [https://bugzilla.redhat.com/show_bug.cgi?id=379811|https://bugzilla.redhat.com/show_bug.cgi?id=379811]


 


============================

==== JBCS-453

* Summary: some httpd tools on windows use a static libraries

* Description:

============================

None

============================

==== JBCS-448

* Summary: mod_proxy_hcheck should be aware of BalancerMember's connectiontimeout parameter and should timeout based on it.

* Description:

============================

When connectiontimeout parameter is given to BalancerMember directive like the following, mod_proxy_hcheck is not aware of the connectiontimeout parameter. Therefore, in case that the network is unreachable, the worker for health-check does not timeout in connectiontimeout seconds even though the worker for real HTTP request to the same backend timeouts in connectiontimeout seconds. mod_proxy_hcheck should be aware of BalancerMember's connectiontimeout parameter and should timeout based on it.

{code}
<VirtualHost *:80>
  ...(snip)...
  ProxyPass /A balancer://webapps/A
</VirtualHost>

<Proxy balancer://webapps>
  BalancerMember http://<host1_ipaddress>:<host1_port> route=jvm1 loadfactor=10 connectiontimeout=15 hcmethod=TCP hcinterval=5 hcpasses=2 hcfails=3
  BalancerMember http://<host2_ipaddress>:<host2_port> route=jvm2 loadfactor=10 connectiontimeout=15 hcmethod=TCP hcinterval=5 hcpasses=2 hcfails=3
  ProxySet stickysession=JSESSIONID|jsessionid
</Proxy>
{code}


============================

==== JBCS-399

* Summary: stickysession parameter specified in ProxyPass line is not enabled

* Description:

============================

In the following reverse proxy and load balancing configuration, stickysession parameter specified in the ProxyPass line is not enabled.
{noformat}
Listen site1.example.com:80

<VirtualHost site1.example.com:80>
  ProxyRequests Off
  ProxyPass /jboss-helloworld balancer://webapps/jboss-helloworld stickysession=JSESSIONID|jsessionid
  ProxyPassReverse /jboss-helloworld balancer://webapps/jboss-helloworld
</VirtualHost>

<Proxy balancer://webapps>
  BalancerMember http://site0.example.com:8180 route=jvm1 loadfactor=10
  BalancerMember http://site0.example.com:8280 route=jvm2 loadfactor=10
</Proxy>
{noformat}

The condition this problem occurs is that stickysession paramter is specified in ProxyPass line in VirtualHost block and the corresponding Proxy block is located outside the VirtualHost block.

This is also reproducible in httpd-2.4.27 as well as JBCS 2.4.23.

============================

==== JBCS-394

* Summary: Apache HTTPD segfaults during postinstall when RHEL in FIPS mode

* Description:

============================

If RHEL server is in FIPS mode, unable to run postinstall for JBCS Apache HTTPD. 

{code}
# unzip jbcs-httpd24-httpd-2.4.23-RHEL6-x86_64.zip
# cd jbcs-httpd24-2.4/httpd/
# ./.postinstall
./.postinstall: line 7:  2761 Aborted                 (core dumped) sbin/openssl genrsa -rand /proc/apm:/proc/cpuinfo:/proc/dma:/proc/filesystems:/proc/interrupts:/proc/ioports:/proc/pci:/proc/rtc:/proc/uptime 1024 > conf/openssl/pki/tls/private/localhost.key 2> /dev/null
./.postinstall: line 26:  2764 Done                    cat  <<EOF
--
SomeState
SomeCity
SomeOrganization
SomeOrganizationalUnit
${FQDN}
root@${FQDN}
EOF

      2765 Aborted                 (core dumped) | OPENSSL_CONF=conf/openssl/pki/tls/openssl.cnf sbin/openssl req -new -key conf/openssl/pki/tls/private/localhost.key -x509 -days 365 -set_serial $RANDOM -out conf/openssl/pki/tls/certs/localhost.crt 2> /dev/null

{code}

============================

==== JBCS-378

* Summary: creating src zips for SP releases too 

* Description:

============================

This bug is for generating src zips for SP2

============================

==== JBCS-375

* Summary: mod_cluster + mod_proxy_wstunnel causes Segmentation Fault

* Description:

============================

Reported by KooV <coolseed hotmail.com];

h3. Description of problem:
mod_cluster + mod_proxy_wstunnel causes segfault

h3. Version-Release number of selected component (if applicable):
RHEL 6.9 x86_64
Jboss Web Server 3.0.3 + SP1
JBoss EAP 6.4.16

This occurred while testing the example https://access.redhat.com/solutions/47442

h3. mod_cluster.conf in httpd
{code}
LoadModule proxy_cluster_module modules/mod_proxy_cluster.so
LoadModule cluster_slotmem_module modules/mod_cluster_slotmem.so
LoadModule manager_module modules/mod_manager.so
LoadModule advertise_module modules/mod_advertise.so

MemManagerFile /software/httpd/cache/mod_cluster

Listen 192.168.0.81:6666
<VirtualHost 192.168.0.81:6666>
    <Directory "/">
        Require all granted
    </Directory>

    KeepAliveTimeout 60
    MaxKeepAliveRequests 0
    ManagerBalancerName mycluster
    EnableMCPMReceive
    ServerAdvertise Off
</VirtualHost>

<Location /mc>
    SetHandler mod_cluster-manager
    Require all granted
</Location>
{code}

h3. mod_proxy_wstunnel.conf in httpd
{code}
LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so
#LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
LoadModule proxy_wstunnel_module modules/mod_proxy_wstunnel.so

<Proxy balancer://TEST_WS>
    BalancerMember ws://192.168.0.81:8080 route=node11
    BalancerMember ws://192.168.0.82:8080 route=node21
    ProxySet stickysession=JSESSIONID|jsessionid
</Proxy>

RewriteEngine On

RewriteCond %{HTTP:Connection} upgrade [NC]
RewriteCond %{HTTP:Upgrade} websocket [NC]
RewriteRule /jboss-websocket-hello/(.*)               balancer://TEST_WS/jboss-websocket-hello/$1 [P,L]
{code}

If you request websocket open, segfault will always occur.

This only happens when using a balancer and not when testing with no balancer.

Similarly, wierd state occurs when using mod_jk + mod_proxy_wstunnel.

If you try to stop httpd in the open state of websocket, it will not be stopped and it will end with SIGKILL.

I think there is something wrong with balancer.

============================

==== JBCS-360

* Summary: _shm file causes the failure of httpd service

* Description:

============================

Customer is migrating from JWS 3.0 to JWS 3.1. Customer has downloaded the jbcs-httpd24 (httpd) V2.4.23 and keeping all the old configuration for jbcs-httpd(httpd.conf, ssl.conf etc). 

While starting the server, they are facing below error message in the error.log:

{noformat}
[Tue May 23 14:47:44.080848 2017] [so:warn] [pid 17935] AH01574: module xml2enc_module is already loaded, skipping
[Tue May 23 14:47:44.080927 2017] [so:warn] [pid 17935] AH01574: module proxy_html_module is already loaded, skipping
[Tue May 23 14:47:44.081134 2017] [core:warn] [pid 17935] AH00111: Config variable ${TheHost} is not defined
[Tue May 23 14:47:44.098388 2017] [auth_digest:notice] [pid 17936] AH01757: generating secret for digest authentication ...
[Tue May 23 14:47:44.098462 2017] [auth_digest:error] [pid 17936] (2)No such file or directory: AH01762: Failed to create shared memory segment on file /run/httpd/authdigest_shm.17936
[Tue May 23 14:47:44.098480 2017] [auth_digest:error] [pid 17936] (2)No such file or directory: AH01760: failed to initialize shm - all nonce-count checking, one-time nonces, and MD5-sess algorithm disabled
[Tue May 23 14:47:44.098492 2017] [:emerg] [pid 17936] AH00020: Configuration Failed, exiting
{noformat}

============================

==== JBCS-315

* Summary: Typo in comment of sample configuration

* Description:

============================

There is a typo in the {{$JBCS_HOME/httpd/conf.d/mod_jk.conf.sample}} configuration sample that I noticed when doing a review of the LB config guide. It also exists in the documentation, I'll fix that one.

The following comment should say "SSL KEY SIZE" instead of "SSK KEY SIZE":

{noformat}
# JkOptions indicates to send SSK KEY SIZE
JkOptions +ForwardKeySize +ForwardURICompat -ForwardDirectories
{noformat}

============================

==== JBCS-255

* Summary: Backport DeflateAlterETag directive to httpd 2.4

* Description:

============================

In httpd 2.4, mod_deflate appends the compression method onto the end of the ETag, causing compressed and uncompressed representations to have unique ETags. Hence, using mod_deflate prevents "304 Not Modified" response.

Please backport [this commit|https://github.com/apache/httpd/commit/20274bdd20ebc66286c5c3f3be334ad91043ae25] which was incorporated in trunk. This provides [DeflateAlterETag|http://httpd.apache.org/docs/trunk/en/mod/mod_deflate.html#deflatealteretag] directive to control how the ETag is modified when using mod_deflate. This configuration can provide a way to mimic old 2.2.x behavior by using  'NoChange' parameter.

Apache community bugzilla: 
https://bz.apache.org/bugzilla/show_bug.cgi?id=39727 / https://bz.apache.org/bugzilla/show_bug.cgi?id=45023

============================

==== JBCS-197

* Summary: BalancerMember directives don't work and cause SegFaults

* Description:

============================

There has been an ongoing discussion about interoperability between BalancerMember and ProxyPass directives and mod_cluster. This is a follow up on MODCLUSTER-391 and especially MODCLUSTER-356.

h3. TL;DR
* BalancerMember directives don't work as expected (at all)
* it is possible to use it to cause SegFault in httpd
* If these directives are *supposed to work*, then I have a wrong configuration or it is a bug to be fixed
* If they are *not supposed to work* in conjunction with mod_cluster, then I should stop trying to test these and remove all ever-failing scenarios from the test suite

h3. Configuration and goal
* two web apps, [^clusterbench.war] and [^tses.war], both deployed on each of two tomcats
* one web app is in excluded contexts (it is  [^tses.war])
* the other one ([^clusterbench]) is registered with mod_cluster balancer
* main server: {{\*:2080}}
* mod_cluster VirtualHost: {{\*:8747}}
* proxyPass BalancerMember VirtualHost {{\*:2081}}
* I want to access [^clusterbench.war] via {{\*:8747}} and {{\*:2080}} (works (/)), and [^tses.war] via {{\*:2081}} (fails (x))
* see [^proxy_test.conf] for BalancerMember configuration (taken from httpd 2.2.26 test run, you must edit Location access)
* see [^mod_cluster.conf] for mod_cluster configuration (taken from httpd 2.2.26 test run, as above)

h3. Test
* (/) check, that only [^clusterbench.war] is registered and everything is cool: [mod_cluster-manager console|https://gist.github.com/Karm/26015dabf446360b0e019da6c907bed5]
* (/) [^clusterbench.war] on mod_cluster VirtualHost works: {{curl http://192.168.122.172:8747/clusterbench/requestinfo}}
* (/) [^clusterbench.war] on main server also works: {{curl http://192.168.122.172:2080/clusterbench/requestinfo}} (it works due to MODCLUSTER-430)
* httpd 2.2.26 / mod_cluster 1.2.13.Final:
** (x) [^tses.war] on BalancerMember ProxyPass VirtualHost fails: {{curl http://192.168.122.172:2081/tses}} with: {noformat}mod_proxy_cluster.c(2374): proxy: byrequests balancer FAILED
proxy: CLUSTER: (balancer://xqacluster). All workers are in error state
{noformat} and it doesn't matter whether I configure the same balancer (qacluster) for both mod_cluster and additional BalancerMemebr directives or if I have two balancers (this case).
** (x) [^clusterbench.war] on BalancerMember ProxyPass VirtualHost sometimes works and sometimes causes SegFault {{curl http://192.168.122.172:2081/clusterbench/requestinfo}} (see below)
* httpd 2.4.23 / mod_cluster 1.3.3.Final:
** (x) [^tses.war] on BalancerMember ProxyPass VirtualHost fails with {{curl http://192.168.122.172:2081/tses}} SegFault, *always* (see below)
** (/) [^clusterbench.war] on BalancerMember ProxyPass VirtualHost works {{curl http://192.168.122.172:2081/clusterbench/requestinfo}}


h3. Intermittent and stable SegFaults

h4. httpd 2.2.26 / mod_cluster 1.2.13.Final (EWS 2.1.1)
With the aforementioned setup, it is possible to cause SegFault roughly in 50% of requests to {{curl http://192.168.122.172:2081/clusterbench/requestinfo}} on httpd 2.2.26 mod_cluster 1.2.13.Final, the rest passes fine and the web app is served.

*Offending line:* [mod_proxy_cluster.c:3843|https://github.com/modcluster/mod_cluster/blob/1.2.13.Final/native/mod_proxy_cluster/mod_proxy_cluster.c#L3843]
*Trace:*
{noformat}
#0  proxy_cluster_pre_request (worker=<optimized out>, balancer=<optimized out>, r=0x5555558be3e0, conf=0x5555558767d8, url=0x7fffffffdd40) at mod_proxy_cluster.c:3843
#1  0x00007ffff0cfe3d6 in proxy_run_pre_request (worker=worker@entry=0x7fffffffdd38, balancer=balancer@entry=0x7fffffffdd30, r=r@entry=0x5555558be3e0, 
    conf=conf@entry=0x5555558767d8, url=url@entry=0x7fffffffdd40) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/modules/proxy/mod_proxy.c:2428
#2  0x00007ffff0d01ef2 in ap_proxy_pre_request (worker=worker@entry=0x7fffffffdd38, balancer=balancer@entry=0x7fffffffdd30, r=r@entry=0x5555558be3e0, 
    conf=conf@entry=0x5555558767d8, url=url@entry=0x7fffffffdd40) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/modules/proxy/proxy_util.c:1512
#3  0x00007ffff0cfeabb in proxy_handler (r=0x5555558be3e0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/modules/proxy/mod_proxy.c:952
#4  0x00005555555805e0 in ap_run_handler (r=0x5555558be3e0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/config.c:157
#5  0x00005555555809a9 in ap_invoke_handler (r=r@entry=0x5555558be3e0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/config.c:376
#6  0x000055555558dc58 in ap_process_request (r=r@entry=0x5555558be3e0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/modules/http/http_request.c:282
#7  0x000055555558aff8 in ap_process_http_connection (c=0x5555558ae2f0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/modules/http/http_core.c:190
#8  0x0000555555587010 in ap_run_process_connection (c=0x5555558ae2f0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/connection.c:43
#9  0x00005555555873b0 in ap_process_connection (c=c@entry=0x5555558ae2f0, csd=<optimized out>) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/connection.c:190
#10 0x0000555555592b5b in child_main (child_num_arg=child_num_arg@entry=0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/mpm/prefork/prefork.c:667
#11 0x0000555555592fae in make_child (s=0x5555557bf880, slot=0) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/mpm/prefork/prefork.c:712
#12 0x0000555555593b6e in ap_mpm_run (_pconf=_pconf@entry=0x5555557ba158, plog=<optimized out>, s=s@entry=0x5555557bf880)
    at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/mpm/prefork/prefork.c:988
#13 0x000055555556b50e in main (argc=8, argv=0x7fffffffe268) at /builddir/build/BUILD/httpd-EWS_2.1.1.CR1/server/main.c:753
{noformat}

h4. httpd 2.4.23 / mod_cluster 1.3.3.Final (JBCS 2.4.23)
With the aforementioned setup, it is *always* possible to SegFault httpd by accessing [^tses.war] on BalancerMember ProxyPass VirtualHos: {{curl http://192.168.122.172:2081/tses}}.

*Offending line:* [mod_proxy_cluster.c:2230|https://github.com/modcluster/mod_cluster/blob/1.3.3.Final/native/mod_proxy_cluster/mod_proxy_cluster.c#L2230]
*Trace:*
{noformat}
#0  0x00007fffe61a598f in internal_find_best_byrequests (balancer=0x55555593ad38, conf=0x555555918dd8, r=0x5555559a6630, domain=0x0, failoverdomain=0, 
    vhost_table=0x5555559a5c98, context_table=0x5555559a5e00, node_table=0x5555559a6088) at mod_proxy_cluster.c:2230
#1  0x00007fffe61a90c8 in find_best_worker (balancer=0x55555593ad38, conf=0x555555918dd8, r=0x5555559a6630, domain=0x0, failoverdomain=0, vhost_table=0x5555559a5c98, 
    context_table=0x5555559a5e00, node_table=0x5555559a6088, recurse=1) at mod_proxy_cluster.c:3457
#2  0x00007fffe61a9f4d in proxy_cluster_pre_request (worker=0x7fffffffdb68, balancer=0x7fffffffdb60, r=0x5555559a6630, conf=0x555555918dd8, url=0x7fffffffdb70)
    at mod_proxy_cluster.c:3825
#3  0x00007fffec2fd9a6 in proxy_run_pre_request (worker=worker@entry=0x7fffffffdb68, balancer=balancer@entry=0x7fffffffdb60, r=r@entry=0x5555559a6630, 
    conf=conf@entry=0x555555918dd8, url=url@entry=0x7fffffffdb70) at mod_proxy.c:2853
#4  0x00007fffec302652 in ap_proxy_pre_request (worker=worker@entry=0x7fffffffdb68, balancer=balancer@entry=0x7fffffffdb60, r=r@entry=0x5555559a6630, 
    conf=conf@entry=0x555555918dd8, url=url@entry=0x7fffffffdb70) at proxy_util.c:1956
#5  0x00007fffec2fe1dc in proxy_handler (r=0x5555559a6630) at mod_proxy.c:1108
#6  0x00005555555aeff0 in ap_run_handler (r=r@entry=0x5555559a6630) at config.c:170
#7  0x00005555555af539 in ap_invoke_handler (r=r@entry=0x5555559a6630) at config.c:434
#8  0x00005555555c5b2a in ap_process_async_request (r=0x5555559a6630) at http_request.c:410
#9  0x00005555555c5e04 in ap_process_request (r=r@entry=0x5555559a6630) at http_request.c:445
#10 0x00005555555c1ded in ap_process_http_sync_connection (c=0x555555950050) at http_core.c:210
#11 ap_process_http_connection (c=0x555555950050) at http_core.c:251
#12 0x00005555555b9470 in ap_run_process_connection (c=c@entry=0x555555950050) at connection.c:42
#13 0x00005555555b99c8 in ap_process_connection (c=c@entry=0x555555950050, csd=<optimized out>) at connection.c:226
#14 0x00007fffec513a30 in child_main (child_num_arg=child_num_arg@entry=0, child_bucket=child_bucket@entry=0) at prefork.c:723
#15 0x00007fffec513c70 in make_child (s=0x55555582d400, slot=slot@entry=0, bucket=bucket@entry=0) at prefork.c:767
#16 0x00007fffec51521d in prefork_run (_pconf=<optimized out>, plog=0x5555558313a8, s=0x55555582d400) at prefork.c:979
#17 0x0000555555592aae in ap_run_mpm (pconf=pconf@entry=0x555555804188, plog=0x5555558313a8, s=0x55555582d400) at mpm_common.c:94
#18 0x000055555558bb18 in main (argc=8, argv=0x7fffffffe1a8) at main.c:783
{noformat}

h3. About the test
This test has always been failing in one way or another: not serving URL (HTTP 404), returning All workers in Error state (HTTP 503). SegFault has been slipping under the radar for some time, because the test ended up on assert earlier in the scenario - on the first HTTP 503.
We should clearly document which BalancerMember integration is supported and which is not. Furthermore, we must not SegFault even if user tries to do something weird, we must log an error message instead.

============================

==== JBCS-163

* Summary: jbcs-httpd24-openssl-perl depends on perl(WWW::Curl::Easy) from base-os optional

* Description:

============================

With currently added openssl into JBCS distribution, there was added also {{jbcs-httpd24-openssl-perl}} package. Although this package is not directly required to make openssl working in JBCS, customers that want to install it will be required to also install {{perl-WWW-Curl}} package from -optional base-os channel as it is a dependency for it. We should put this information into our JBCS documentation somewhere.

----
Original Jira description follows:

With openssl currently added into JBCS distribution, there is also introduced new dependency on a package from base-os -optional channel ({{rhel-X-server-optional-rpms}}). More specifically, there is added {{jbcs-httpd24-openssl-perl}} package, which requires {{perl-WWW-Curl}} which is available from -optional channel.

As mentioned also in JBCS-31, we don't like to depend on packages from optional channel, because of [this|https://access.redhat.com/support/offerings/production/soc], [this|https://access.redhat.com/support/offerings/production/scope_moredetail] and [this|https://access.redhat.com/articles/1150793]; excerpt from latest link:
{quote}
Software packages in the Optional and Supplementary channels are not supported, nor are the ABIs guaranteed.
{quote}

Marking this as a blocker as I believe this should be determined before release.

============================

== Release Note: 5.backlog.GA

* Total Jiras: 6

==== JWS-1374

* Summary: tomcat doesn't clean own logs by logrotate

* Description:

============================

+++ This bug was initially created as a clone of Bug #1701249 +++
Description of problem:
I look at system where the tomcat has run for long time. And a lot of old log files exist, I suppose that the logs should be cleaned.

{code:bash}
$ ls  /var/log/tomcat/
total 931192    <<<<<<<<<<<<<see the high amount of files (!)
-rw-r--r-- 1 tomcat tomcat     18938 Sep 28  2016 catalina.2016-09-28.log
-rw-r--r-- 1 tomcat tomcat     28648 Sep 29  2016 catalina.2016-09-29.log
-rw-r--r-- 1 tomcat tomcat      9461 Sep 30  2016 catalina.2016-09-30.log
-rw-r--r-- 1 tomcat tomcat      9675 Oct  1  2016 catalina.2016-10-01.log
-rw-r--r-- 1 tomcat tomcat      9925 Oct  3  2016 catalina.2016-10-03.log
-rw-r--r-- 1 tomcat tomcat      9478 Oct  4  2016 catalina.2016-10-04.log
-rw-r--r-- 1 tomcat tomcat     67972 Oct  5  2016 catalina.2016-10-05.log
-rw-r--r-- 1 tomcat tomcat      9670 Oct  6  2016 catalina.2016-10-06.log
-rw-r--r-- 1 tomcat tomcat      9674 Oct  7  2016 catalina.2016-10-07.log
-rw-r--r-- 1 tomcat tomcat      9674 Oct  8  2016 catalina.2016-10-08.log
-rw-r--r-- 1 tomcat tomcat      9674 Oct  9  2016 catalina.2016-10-09.log
-rw-r--r-- 1 tomcat tomcat      9727 Oct 10  2016 catalina.2016-10-10.log
-rw-r--r-- 1 tomcat tomcat      9477 Oct 11  2016 catalina.2016-10-11.log
-rw-r--r-- 1 tomcat tomcat      9674 Oct 12  2016 catalina.2016-10-12.log
-rw-r--r-- 1 tomcat tomcat      9675 Oct 13  2016 catalina.2016-10-13.log
-rw-r--r-- 1 tomcat tomcat       250 Oct 17  2016 catalina.2016-10-17.log
-rw-r--r-- 1 tomcat tomcat       250 Oct 24  2016 catalina.2016-10-24.log
-rw-r--r-- 1 tomcat tomcat       250 Oct 31  2016 catalina.2016-10-31.log
-rw-r--r-- 1 tomcat tomcat     59532 Nov  1  2016 catalina.2016-11-01.log
-rw-r--r-- 1 tomcat tomcat     28430 Nov  4  2016 catalina.2016-11-04.log
-rw-r--r-- 1 tomcat tomcat       250 Nov  7  2016 catalina.2016-11-07.log
-rw-r--r-- 1 tomcat tomcat       250 Nov 14  2016 catalina.2016-11-14.log
-rw-r--r-- 1 tomcat tomcat    237373 Nov 17  2016 catalina.2016-11-17.log
-rw-r--r-- 1 tomcat tomcat       250 Nov 21  2016 catalina.2016-11-21.log
-rw-r--r-- 1 tomcat tomcat       250 Nov 28  2016 catalina.2016-11-28.log
-rw-r--r-- 1 tomcat tomcat     34337 Nov 29  2016 catalina.2016-11-29.log
-rw-r--r-- 1 tomcat tomcat     34003 Nov 30  2016 catalina.2016-11-30.log
-rw-r--r-- 1 tomcat tomcat     61087 Dec  1  2016 catalina.2016-12-01.log
-rw-r--r-- 1 tomcat tomcat     68240 Dec  2  2016 catalina.2016-12-02.log
-rw-r--r-- 1 tomcat tomcat     34581 Dec  3  2016 catalina.2016-12-03.log
-rw-r--r-- 1 tomcat tomcat     30584 Dec  4  2016 catalina.2016-12-04.log
-rw-r--r-- 1 tomcat tomcat     32703 Dec  5  2016 catalina.2016-12-05.log
-rw-r--r-- 1 tomcat tomcat     35446 Dec  6  2016 catalina.2016-12-06.log
-rw-r--r-- 1 tomcat tomcat     35461 Dec  7  2016 catalina.2016-12-07.log
-rw-r--r-- 1 tomcat tomcat     35162 Dec  8  2016 catalina.2016-12-08.log
-rw-r--r-- 1 tomcat tomcat     79718 Dec  9  2016 catalina.2016-12-09.log
-rw-r--r-- 1 tomcat tomcat     35161 Dec 10  2016 catalina.2016-12-10.log
-rw-r--r-- 1 tomcat tomcat     31463 Dec 11  2016 catalina.2016-12-11.log
-rw-r--r-- 1 tomcat tomcat     33837 Dec 12  2016 catalina.2016-12-12.log
-rw-r--r-- 1 tomcat tomcat     34005 Dec 13  2016 catalina.2016-12-13.log
.
snip
.
-rw-r--r-- 1 tomcat tomcat         0 Mar  1 20:10 manager.2019-03-01.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  2 20:11 manager.2019-03-02.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  3 20:11 manager.2019-03-03.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  4 20:10 manager.2019-03-04.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  5 20:11 manager.2019-03-05.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  6 07:32 manager.2019-03-06.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  7 20:09 manager.2019-03-07.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  8 20:11 manager.2019-03-08.log
-rw-r--r-- 1 tomcat tomcat         0 Mar  9 20:09 manager.2019-03-09.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 10 20:10 manager.2019-03-10.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 11 20:11 manager.2019-03-11.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 12 20:10 manager.2019-03-12.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 13 20:10 manager.2019-03-13.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 14 13:46 manager.2019-03-14.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 15 20:10 manager.2019-03-15.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 16 20:10 manager.2019-03-16.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 17 20:11 manager.2019-03-17.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 18 20:12 manager.2019-03-18.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 19 20:12 manager.2019-03-19.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 20 20:11 manager.2019-03-20.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 21 20:10 manager.2019-03-21.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 22 20:11 manager.2019-03-22.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 23 20:09 manager.2019-03-23.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 24 20:11 manager.2019-03-24.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 25 20:11 manager.2019-03-25.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 26 20:12 manager.2019-03-26.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 27 20:12 manager.2019-03-27.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 28 10:19 manager.2019-03-28.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 29 20:11 manager.2019-03-29.log
-rw-r--r-- 1 tomcat tomcat         0 Mar 31 20:12 manager.2019-03-31.log
-rw-r--r-- 1 tomcat tomcat         0 Apr  1 07:11 manager.2019-04-01.log
-rw-r--r-- 1 tomcat tomcat         0 Apr  2 09:30 manager.2019-04-02.log
-rw-r--r-- 1 tomcat tomcat         0 Apr  3 14:26 manager.2019-04-03.log
-rw-r--r-- 1 tomcat tomcat         0 Apr  4 07:30 manager.2019-04-04.log
{code}

{code:bash}
$ cat etc/logrotate.d/tomcat 
/var/log/tomcat/catalina.out {
    copytruncate
    weekly
    rotate 52
    compress
    missingok
    create 0644 tomcat tomcat
}
{code}

Version-Release number of selected component (if applicable):

{code:bash}
$ cat etc/redhat-release 
Red Hat Enterprise Linux Server release 7.6 (Maipo)
$ grep -i tomcat installed-rpms 
lh555.server.raiffeisen.ch-tomcat-1.0-1.noarch              Tue Feb 13 07:04:46 2018
tomcat-7.0.76-8.el7_5.noarch                                Wed Nov 14 09:19:38 2018
tomcat-el-2.2-api-7.0.76-8.el7_5.noarch                     Wed Nov 14 09:18:39 2018
tomcat-jsp-2.2-api-7.0.76-8.el7_5.noarch                    Wed Nov 14 09:18:05 2018
tomcat-lib-7.0.76-8.el7_5.noarch                            Wed Nov 14 09:18:40 2018
tomcat-servlet-3.0-api-7.0.76-8.el7_5.noarch                Wed Nov 14 09:18:04 2018
{code}

How reproducible:
alawys

Steps to Reproduce:
1. run tomcat long time
2. show content of directory with logs 

Actual results:
A lot of logs files

Expected results:
old log files was compressed or remove

Additional info:

The same bug was opened for rhel 6.9 but was closed as WONTFIX due to "Red Hat Enterprise Linux 6 is in the Production 3 Phase."

============================

==== JWS-1320

* Summary: ASF BZ 63199: Tomcat Native: sslsocket handshake JVM crash

* Description:

============================

See https://bz.apache.org/bugzilla/show_bug.cgi?id=63199

TODO:

 * reproduce
 * debug

============================

==== JWS-1274

* Summary: Review and Revise the JWS Documentation Set

* Description:

============================

Full review of JWS documentation set.
- Test for technical accuracy
- Modularise
- Bring docs up to current internal standards and prepare for future docs build/deployment changes:
-- [CCS: FY19 Goals and Initiatives|https://mojo.redhat.com/docs/DOC-1166815]
-- [Modular Documentation Project|https://mojo.redhat.com/docs/DOC-1103812]


============================

==== JWS-1272

* Summary: Create the JWS Security Hardening Guide

* Description:

============================

Create a Security hardening Guide based on customer requirement

============================

==== JWS-855

* Summary: Poor UX of Native libraries installation for Tomcat Embedded users

* Description:

============================

In order to use APR connector or NIO+JSSE+OpenSSL connector in an embedded application, one has to:
 * download standalone Tomcat distribution zip
 * extract lib directory from that zip
 * set LD_LIBRARY_PATH
 * run one's embedded web application

e.g. 
{code}
unzip jws-application-server-4.0.0-CR1-RHEL7-x86_64.zip jws-4.0/tomcat/lib* -d /tmp/tomcat-libs
export LD_LIBRARY_PATH=/tmp/tomcat-libs/jws-4.0/tomcat/lib/
java -jar target/embedded-tomcat-app.jar
{code}

It is exceedingly cumbersome. Could we device a better way? Is it feasible to package .so files inside some jar and put that jar in the Maven repository and let the web app to depend on it? For example, see [wildfly-openssl-1.0.3.Final-SNAPSHOT.jar|https://ci.modcluster.io/job/wildfly-openssl-windows/arch=64,label=w2k12r2/lastSuccessfulBuild/artifact/combined/target/wildfly-openssl-1.0.3.Final-SNAPSHOT.jar] and its:
{code}
    win-x86_64/
    win-x86_64/wfssl.dll
    win-x86_64/wfssl.exp
    win-x86_64/wfssl.lib
{code}


h3. Simplified example
While the size of the Wildfly-OpenSSL project is quite intimidating, one can take a look at a dead-simple Linux+Windows demo hobby project streamlining the procedure to the bare minimum: https://github.com/Karm/CRC64Java

============================

==== JWS-683

* Summary: Add documentation in docs-zip for mod_cluster

* Description:

============================

The docs zip distribution provided by JWS (e.g. jws-docs-3.1.0.zip) includes an empty mod_cluster directory. That directory should be populated with the upstream documentation for users.

============================



----------
Report time: 2019-07-19 21:06:12.634618


